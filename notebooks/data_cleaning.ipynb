{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning and Preprocessing\n",
        "\n",
        "This notebook demonstrates the data cleaning and preprocessing steps for retail review sentiment analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Raw Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load raw reviews\n",
        "df = pd.read_csv('../data/raw_reviews.csv')\n",
        "print(f\"Loaded {len(df)} reviews\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "    \"\"\"Tokenize text and lemmatize tokens, removing stopwords.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [\n",
        "        lemmatizer.lemmatize(token) \n",
        "        for token in tokens \n",
        "        if token not in stop_words and len(token) > 2\n",
        "    ]\n",
        "    \n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "print(\"Cleaning functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "print(\"Handling missing values...\")\n",
        "df = df.dropna(subset=['review_text'])\n",
        "print(f\"Reviews after removing missing values: {len(df)}\")\n",
        "\n",
        "# Remove duplicates\n",
        "print(\"\\nRemoving duplicates...\")\n",
        "initial_count = len(df)\n",
        "df = df.drop_duplicates(subset=['review_text'])\n",
        "duplicates_removed = initial_count - len(df)\n",
        "print(f\"Removed {duplicates_removed} duplicate reviews\")\n",
        "print(f\"Reviews remaining: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean text\n",
        "print(\"Cleaning text...\")\n",
        "df['cleaned_text'] = df['review_text'].apply(clean_text)\n",
        "df = df[df['cleaned_text'].str.len() > 0]\n",
        "print(f\"Reviews after cleaning: {len(df)}\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\nExample of cleaned text:\")\n",
        "print(f\"Original: {df.iloc[0]['review_text']}\")\n",
        "print(f\"Cleaned: {df.iloc[0]['cleaned_text']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize and lemmatize\n",
        "print(\"Tokenizing and lemmatizing...\")\n",
        "df['processed_text'] = df['cleaned_text'].apply(tokenize_and_lemmatize)\n",
        "df = df[df['processed_text'].str.len() > 0]\n",
        "print(f\"Final processed reviews: {len(df)}\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\nExample of processed text:\")\n",
        "print(f\"Cleaned: {df.iloc[0]['cleaned_text']}\")\n",
        "print(f\"Processed: {df.iloc[0]['processed_text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data\n",
        "df.to_csv('../data/processed_reviews.csv', index=False)\n",
        "print(\"Processed data saved to ../data/processed_reviews.csv\")\n",
        "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "df.head()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
